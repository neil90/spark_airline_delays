{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 1.6.1\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.5.1 (default, Feb 16 2016 09:49:46)\n",
      "SparkContext available as sc, HiveContext available as sqlContext.\n"
     ]
    }
   ],
   "source": [
    "#Enable Spark Context\n",
    "#System Variables - SPARK_HOME, PACKAGES, PYSPARK_SUBMIT_ARGS\n",
    "#PACKAGES=com.databricks:spark-csv_2.11:1.4.0\n",
    "#PYSUBMIT_SUBMIT_ARGS=--packages %PACKAGES% pyspark-shell \n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "spark_home = os.environ.get('SPARK_HOME', None)\n",
    "sys.path.insert(0, spark_home + \"/python\")\n",
    "sys.path.insert(0, os.path.join(spark_home, 'python/lib/py4j-0.8.2.1-src.zip'))\n",
    "\n",
    "filename = os.path.join(spark_home, 'python/pyspark/shell.py')\n",
    "exec(compile(open(filename, \"rb\").read(), filename, 'exec'))\n",
    "\n",
    "spark_release_file = spark_home + \"/RELEASE\"\n",
    "\n",
    "if os.path.exists(spark_release_file) and \"Spark 1.6.1\" in open(spark_release_file).read():\n",
    "    pyspark_submit_args = os.environ.get(\"PYSPARK_SUBMIT_ARGS\", \"\")\n",
    "    if not \"pyspark-shell\" in pyspark_submit_args: \n",
    "        pyspark_submit_args += \" pyspark-shell\"\n",
    "        os.environ[\"PYSPARK_SUBMIT_ARGS\"] = pyspark_submit_args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Load Airline and Weather data and make 1 DF for each Airline and Weather\n",
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)\n",
    "\n",
    "df_airline_2007 = sqlContext.read.format('com.databricks.spark.csv') \\\n",
    "    .options(header='true', inferschema='true') \\\n",
    "    .load(r\"C:\\Users\\Neil-Laptop\\Documents\\datasets\\airline_2007.csv\")\n",
    "\n",
    "df_airline_2008 = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='true', inferschema='true') \\\n",
    "    .load(r\"C:\\Users\\Neil-Laptop\\Documents\\datasets\\airline_2008.csv\")\n",
    "    \n",
    "    \n",
    "df_weather_2007 = sqlContext.read.format('com.databricks.spark.csv') \\\n",
    "    .options(header='false', inferschema='true') \\\n",
    "    .load(r\"C:\\Users\\Neil-Laptop\\Documents\\datasets\\weather_2007.csv\")\n",
    "\n",
    "df_weather_2008 = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "    .options(header='false', inferschema='true') \\\n",
    "    .load(r\"C:\\Users\\Neil-Laptop\\Documents\\datasets\\weather_2008.csv\") \n",
    "    \n",
    "    \n",
    "df_airline_raw = df_airline_2007.unionAll(df_airline_2008)\n",
    "df_weather_raw = df_weather_2007.unionAll(df_weather_2008)\n",
    "#df = sqlContext.csvcoload(r\"C:\\Users\\Neil-Laptop\\Downloads\\Parking_Citations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Function to create year,month,day into date for airline to join on to weather\n",
    "def to_date(year,month,day): \n",
    "    dt = \"%04d%02d%02d\" % (year, month, day)\n",
    "    return dt\n",
    "\n",
    "sqlContext.udf.register(\"to_date\", to_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Function to discrentize time in airline\n",
    "def discretize_tod(val):\n",
    "    hour = int(val[:2])\n",
    "    if hour < 8:\n",
    "        return 0\n",
    "    if hour < 16:\n",
    "        return 1\n",
    "    return 2\n",
    "\n",
    "sqlContext.udf.register(\"discretize_tod\", discretize_tod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_airline_raw.registerTempTable(\"df_airpline_raw\")\n",
    "df_weather_raw.registerTempTable(\"df_weather_raw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create Final Airline transformation\n",
    "df_airline = sqlContext.sql(\"\"\"SELECT \n",
    "                            Year as year, Month as month, DayofMonth as day, DayOfWeek as dow,\n",
    "                            CarrierDelay as carrier, Origin as origin, Dest as dest, Distance as distance, \n",
    "                            discretize_tod(DepTime) as tod, CASE WHEN DepDelay >= 15 THEN 1 ELSE 0 END as delay, \n",
    "                            to_date(Year, Month, DayofMonth) As date \n",
    "                            FROM df_airpline_raw\n",
    "                            WHERE Cancelled = 0 AND Origin = 'ORD'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create Base Weather Transformation Table\n",
    "df_weather = sqlContext.sql(\"\"\"SELECT \n",
    "                                C0 AS station,\n",
    "                                C1 As date,\n",
    "                                C2 As metric,\n",
    "                                C3 As value, \n",
    "                                C4 As t1, \n",
    "                                C5 As t2, \n",
    "                                C6 As t3, \n",
    "                                C7 As time\n",
    "                                FROM df_weather_raw\n",
    "                                \"\"\")\n",
    "\n",
    "#Create Tmin and Tmax Weather DF\n",
    "df_weather.registerTempTable(\"df_weather\")\n",
    "\n",
    "#Create DFs for Weather Tmin and Tmax Values \n",
    "df_weather_tmin = sqlContext.sql(\"\"\"SELECT \n",
    "                                        date, \n",
    "                                        value as temp_min \n",
    "                                    FROM df_weather \n",
    "                                    WHERE station = 'USW00094846' \n",
    "                                    AND metric = 'TMIN'\"\"\")\n",
    "                                    \n",
    "df_weather_tmax = sqlContext.sql(\"\"\"SELECT \n",
    "                                        date, \n",
    "                                        value as temp_max \n",
    "                                    FROM df_weather \n",
    "                                    WHERE station = 'USW00094846' \n",
    "                                    AND metric = 'TMAX'\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Join Airline with Weather Tmin and Tmax Dataframes\n",
    "df_airline_tmin = df_airline.join(df_weather_tmin, \n",
    "                                  df_weather_tmin.date == df_airline.date, \n",
    "                                  \"inner\").drop(df_weather_tmin.date)\n",
    "\n",
    "df_airline_tmin_and_tmax = df_airline_tmin.join(df_weather_tmax, \n",
    "                                                df_weather_tmax.date == df_airline_tmin.date, \n",
    "                                                \"inner\").drop(df_weather_tmax.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[delay: int, year: int, month: int, day: int, dow: int, tod: int, distance: int, temp_min: int, temp_max: int]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_airline_tmin_and_tmax.registerTempTable(\"df_airline_tmin_and_tmax\")\n",
    "df_all = sqlContext.sql(\"\"\"SELECT \n",
    "                                delay,\n",
    "                                year,\n",
    "                                month, \n",
    "                                day, \n",
    "                                dow, \n",
    "                                cast (tod AS int) tod, \n",
    "                                distance, \n",
    "                                temp_min, \n",
    "                                temp_max\n",
    "                            FROM df_airline_tmin_and_tmax\"\"\")\n",
    "\n",
    "#Cache Dataframe because we split it later on\n",
    "df_all.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+---+---+---+--------+--------+--------+\n",
      "|delay|year|month|day|dow|tod|distance|temp_min|temp_max|\n",
      "+-----+----+-----+---+---+---+--------+--------+--------+\n",
      "|    0|2007|    5| 24|  4|  2|     925|     172|     311|\n",
      "|    1|2007|    5| 24|  4|  2|     316|     172|     311|\n",
      "|    1|2007|    5| 24|  4|  2|     316|     172|     311|\n",
      "|    0|2007|    5| 24|  4|  1|     925|     172|     311|\n",
      "|    0|2007|    5| 24|  4|  2|     316|     172|     311|\n",
      "|    0|2007|    5| 24|  4|  2|     316|     172|     311|\n",
      "|    1|2007|    5| 24|  4|  1|     316|     172|     311|\n",
      "|    0|2007|    5| 24|  4|  1|     316|     172|     311|\n",
      "|    0|2007|    5| 24|  4|  2|     316|     172|     311|\n",
      "|    0|2007|    5| 24|  4|  1|     654|     172|     311|\n",
      "+-----+----+-----+---+---+---+--------+--------+--------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_all.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Linear Regression\n",
    "#import necessary librarys\n",
    "from pyspark.mllib.regression import  LabeledPoint\n",
    "from pyspark.mllib.tree import DecisionTree, RandomForest\n",
    "from pyspark.mllib.linalg import DenseVector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create labeledPoint Parser\n",
    "def parseDF(row):\n",
    "    values = [row.delay, row.month, row.day, row.dow, row.tod, row.distance, row.temp_min, row.temp_max]\n",
    "    return LabeledPoint(values[0], DenseVector(values[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert Dataframes to LabeledPoint for modeling\n",
    "train_data = df_all.filter(\"year=2007\").map(parseDF)\n",
    "test_data = df_all.filter(\"year=2008\").map(parseDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Train Models\n",
    "modelCART = DecisionTree.trainClassifier(train_data, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                     impurity='gini', maxDepth=5)\n",
    "\n",
    "modelRF = RandomForest.trainClassifier(train_data, numClasses=2, categoricalFeaturesInfo={},\n",
    "                                      numTrees=500, impurity='gini', maxDepth=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apply CART model on Test Data\n",
    "predictionsCART = modelCART.predict(test_data.map(lambda x: x.features))\n",
    "predictionsAndLabelsCARTRDD = predictionsCART.zip(test_data.map(lambda lp: lp.label))\n",
    "predictionsAndLabelsCART = predictionsAndLabelsCARTRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Apply CART model on Test Data\n",
    "predictionsRF = modelRF.predict(test_data.map(lambda x: x.features))\n",
    "predictionsAndLabelsRFRDD = predictionsRF.zip(test_data.map(lambda lp: lp.label))\n",
    "predictionsAndLabelsRF = predictionsAndLabelsRFRDD.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DenseMatrix([[ 229130.,   10764.],\n",
      "             [  87559.,    7877.]])\n"
     ]
    }
   ],
   "source": [
    "#Create confusion matrix\n",
    "#Spark COnfusion Matrix can't interface with it\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "metrics = MulticlassMetrics(predictionsAndLabelsCARTRDD)\n",
    "\n",
    "print(metrics.confusionMatrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Instead due Pandas crosstab to create confusion matrix instead\n",
    "import pandas as pd\n",
    "\n",
    "#Create function\n",
    "\n",
    "def confusion_matrix(predAndLabel):\n",
    "    y_actual = pd.Series([x for x, y in predAndLabel], name = 'Actual')\n",
    "    y_pred = pd.Series([y for x, y in predAndLabel], name = 'Predicted')\n",
    "    \n",
    "    matrix = pd.crosstab(y_actual,y_pred)\n",
    "    accuracy = (matrix[0][0] + matrix[1][1])/ \\\n",
    "                (matrix[0][0] + matrix[0][1] + matrix[1][0] + matrix[1][1])\n",
    "\n",
    "    return matrix, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CART Confusion Matrix:\n",
      "Predicted     0.0    1.0\n",
      "Actual                  \n",
      "0.0        229130  87559\n",
      "1.0         10764   7877\n",
      "\n",
      "CART Model Accuracy: 0.7067873438105746\n"
     ]
    }
   ],
   "source": [
    "#CART Confusion Matrix and Model Accuracy\n",
    "df_confusion_CART, accuracy_CART = confusion_matrix(predictionsAndLabelsCART)\n",
    "\n",
    "print('CART Confusion Matrix:')\n",
    "print(df_confusion_CART)\n",
    "print('\\nCART Model Accuracy: {0}'.format(accuracy_CART))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Confusion Matrix:\n",
      "Predicted     0.0    1.0\n",
      "Actual                  \n",
      "0.0        238077  93466\n",
      "1.0          1817   1970\n",
      "\n",
      "RF Model Accuracy: 0.7158530402886709\n"
     ]
    }
   ],
   "source": [
    "#RandomForest Confusion Matrix and Model Accuracy\n",
    "df_confusion_RF, accuracy_RF = confusion_matrix(predictionsAndLabelsRF)\n",
    "\n",
    "print('RF Confusion Matrix:')\n",
    "print(df_confusion_RF)\n",
    "print('\\nRF Model Accuracy: {0}'.format(accuracy_RF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#It looks like RF Model is better\n",
    "#And with more parameter tuning it can get better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
